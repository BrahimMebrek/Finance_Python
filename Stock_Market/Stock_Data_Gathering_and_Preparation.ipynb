{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finance with Python - Stock Data Gathering and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is the second part of our Stock Data Manipulation Study.\n",
    "\n",
    "It contains contains an automation of the data gathering process by :\n",
    "- Scrapping the tickers symbols of the **S&P 500 index** companies.\n",
    "- Gathering the Stock prices of these companies using the **Yahoo! Finance API**.\n",
    "- Merging the Adjacent Closing Price of each companie into one dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defintions\n",
    "\n",
    "#### S&P 500\n",
    "The S&P 500, or simply the S&P, is a stock market index that measures the stock performance of 500 large companies listed on stock exchanges in the United States. It is one of the most commonly followed equity indices, and many consider it to be one of the best representations of the U.S. stock market.\n",
    "\n",
    "Source : https://en.wikipedia.org/wiki/S%26P_500_Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Scientific computing\n",
    "import pandas as pd # Data analysis and manipulation\n",
    "\n",
    "\n",
    "from datetime import datetime as dt  # Dates and times manipulations\n",
    "\n",
    "import pandas_datareader.data as web # Up to date remote data access for pandas \n",
    "                                     # pip install pandas-datareader\n",
    "\n",
    "import bs4 as bs # Pulling data out of HTML and XML files\n",
    "import pickle    # Serializing and de-serializing a Python object structure.\n",
    "import requests  # Send HTTP requests easily\n",
    "import os        # operating system dependent functionality.\n",
    "\n",
    "import yfinance as yf # Historical market data from Yahoo! finance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping the Tickers symbols of the S&P 500 index companies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will : \n",
    "- Send an HTTP Request to the wikipedia page **List of the S&P 500 companies** : https://en.wikipedia.org/wiki/List_of_S%26P_500_companies .\n",
    "- Scrap the tickers data out of the HTML using BeautifulSoup4.\n",
    "- serialize a Python object using Pickle dump function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to get the 500 tickers of the S&P compagnies\n",
    "def get_sp500_tickers():\n",
    "    \n",
    "    # Sending a requests to the wikipedia page and storing the response\n",
    "    resp = requests.get('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "    \n",
    "    # Creating the BeautifulSoup object, using the text of the response and lxml as a parser \n",
    "    soup = bs.BeautifulSoup(resp.text, 'lxml')\n",
    "    \n",
    "    # Find the table data by class or id (Preferable)\n",
    "    #table = soup.find('table', {'class':'wikitable sortable'})\n",
    "    table = soup.find('table', {'id': 'constituents'})\n",
    "    \n",
    "    # Creating an empty list to store the tickers\n",
    "    tickers = []\n",
    "    \n",
    "    # Looping through all the table rows, except the first table row, it contains the title of the columns\n",
    "    for row in table.findAll('tr')[1:]:\n",
    "        \n",
    "        # Assign the first column (table data)[0] of each row to a ticker variable\n",
    "        # Some of the stock tickers output a period '.' instead of a hyphen '-' (Yahoo! Finance API)\n",
    "        ticker = row.findAll('td')[0].text.replace('.', '-')\n",
    "        \n",
    "        # Getting rid of the \\n\n",
    "        ticker = ticker[:-1]\n",
    "    \n",
    "        # Append the ticker to the tickers list\n",
    "        tickers.append(ticker)\n",
    "    \n",
    "    # Creating the pickle file, with (write binary) mode\n",
    "    with open('sp500_tickers.pickle', 'wb') as file:\n",
    "        \n",
    "        # Dumping the tickers into the pickle file \n",
    "        pickle.dump(tickers, file)\n",
    "    \n",
    "    return tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MMM', 'ABT', 'ABBV', 'ABMD', 'ACN', 'ATVI', 'ADBE', 'AMD', 'AAP', 'AES']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the first 10 tickers symbols \n",
    "get_sp500_tickers()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering the Stock prices using the Yahoo! Finance API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will :\n",
    "- Create a \"data\" directory if it doesn't exist.\n",
    "- De-serialize the Pickle file using the load function in order to get the latest tickers in the S&P 500 list.\n",
    "- Gather the data of each companie from the Yahoo! Finance API using the get_data_yahoo() function.\n",
    "- Store the gathered data in a ticket.csv file.\n",
    "\n",
    "#### Gathering the 500 companies stock prices requires a lot of computational time, you might need to restart this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to get the sp500 data from yahoo and stores it locally \n",
    "def get_sp500_data(start = dt(2010, 1, 1), end = dt.now()):\n",
    "  \n",
    "    # Opening the sp500_tickers.pickle file with (read binary) mode\n",
    "    with open('sp500_tickers.pickle', 'rb') as file:\n",
    "            \n",
    "        # Loading the tickers from the file\n",
    "        tickers = pickle.load(file)\n",
    "    \n",
    "    # Cheking if the data directory exists, else creates it\n",
    "    if not os.path.exists('data'):\n",
    "        os.makedirs('data')\n",
    "        \n",
    "    # Looping through the symbols\n",
    "    # Add [:int < 500] to reduce the amount of data and time \n",
    "    for ticker in tickers:\n",
    "        \n",
    "        # Cheking if the ticker.csv file does not exist \n",
    "        if not os.path.exists('data/{}.csv'.format(ticker)):\n",
    "            \n",
    "            # Get the data from Yahoo\n",
    "            df = web.get_data_yahoo(ticker, start, end)\n",
    "             \n",
    "            # We reset the actual index\n",
    "            df.reset_index(inplace=True)\n",
    "            \n",
    "            # We set in Date as the index\n",
    "            df.set_index(\"Date\", inplace=True)\n",
    "            \n",
    "            # Saving the ticker.csv file\n",
    "            df.to_csv('data/{}.csv'.format(ticker))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the starting and ending date\n",
    "start = dt(2010, 1, 1)\n",
    "end = dt.now() # 2020-03-27\n",
    "\n",
    "# Gathering the sp500 data\n",
    "get_sp500_data(start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging all the S&P 500 data into one DataFrame "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will:\n",
    "- De-serialize the Pickle file in order to get the latest tickers in the S&P 500 list, not all the files in the data directory.\n",
    "- Read every dataset, store the Adjacent Closing Price and merge it with the previous one.\n",
    "- Create one S&P 500 Adjacent Closing Price Dataset and save it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a funtion that will merge all the datasets into one\n",
    "def merge_data():\n",
    "    \n",
    "    # Opening the sp500_tickers.pickle file with (read binary) mode\n",
    "    with open('sp500_tickers.pickle', 'rb') as file:\n",
    "            \n",
    "        # Loading the tickers from the file\n",
    "        tickers = pickle.load(file)\n",
    "    \n",
    "    # Creating an empty dataframe to store all the data    \n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # Looping through the tickers\n",
    "    for ticker in tickers:\n",
    "        \n",
    "        # Reading the symbol.csv file\n",
    "        df_ticker = pd.read_csv('data/{}.csv'.format(ticker))\n",
    "        \n",
    "        # Setting the index to Date\n",
    "        df_ticker.set_index('Date', inplace = True)\n",
    "        \n",
    "        # Renaming the Adj Close to be the symbol\n",
    "        df_ticker.rename(columns = {'Adj Close' : ticker}, inplace = True)\n",
    "        \n",
    "        #Dropping the unwanted columns\n",
    "        df_ticker.drop(['Open', 'High', 'Low', 'Close', 'Volume'], axis = 1, inplace = True)\n",
    "        \n",
    "        # Joining the dataframe with the main_df\n",
    "        df = df.join(df_ticker, how = 'outer', sort = False)\n",
    "      \n",
    "    print(df.head())\n",
    "        \n",
    "    # Creating the sp500_joined_closes.csv file\n",
    "    df.to_csv('sp500_adj_close.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  MMM        ABT  ABBV  ABMD        ACN       ATVI       ADBE  \\\n",
      "Date                                                                            \n",
      "2010-01-04  63.519070  18.229385   NaN  8.74  33.871223  10.193225  37.090000   \n",
      "2010-01-05  63.121239  18.082108   NaN  8.53  34.080551  10.211267  37.700001   \n",
      "2010-01-06  64.016403  18.182526   NaN  8.40  34.442860  10.157144  37.619999   \n",
      "2010-01-07  64.062294  18.333153   NaN  8.40  34.410656   9.913588  36.889999   \n",
      "2010-01-08  64.513718  18.426876   NaN  8.23  34.273785   9.832404  36.689999   \n",
      "\n",
      "             AMD        AAP        AES  ...       WYNN        XEL        XRX  \\\n",
      "Date                                    ...                                    \n",
      "2010-01-04  9.70  39.293575  10.778586  ...  41.963718  14.407637  19.292725   \n",
      "2010-01-05  9.71  39.060036  10.668198  ...  44.515926  14.236770  19.315083   \n",
      "2010-01-06  9.57  39.400608  10.557810  ...  43.932011  14.264105  19.136232   \n",
      "2010-01-07  9.47  39.390865  10.565696  ...  44.870213  14.202594  19.225660   \n",
      "2010-01-08  9.43  39.546574  10.865317  ...  44.548744  14.209433  19.158592   \n",
      "\n",
      "                 XLNX  XYL        YUM       ZBRA        ZBH       ZION  ZTS  \n",
      "Date                                                                         \n",
      "2010-01-04  20.047621  NaN  19.480558  28.670000  55.805180  12.080154  NaN  \n",
      "2010-01-05  19.794851  NaN  19.413939  28.620001  57.571747  12.506084  NaN  \n",
      "2010-01-06  19.660570  NaN  19.275148  28.400000  57.553162  13.593570  NaN  \n",
      "2010-01-07  19.463099  NaN  19.269602  27.690001  58.873451  15.116047  NaN  \n",
      "2010-01-08  19.747461  NaN  19.275148  27.600000  57.636837  14.871366  NaN  \n",
      "\n",
      "[5 rows x 505 columns]\n"
     ]
    }
   ],
   "source": [
    "# Merging the data\n",
    "merge_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The missing values (NaN) mean that the companies were not listed on the stock exchange or were not part of the S&P 500 Index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Created by MEBREK Brahim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
